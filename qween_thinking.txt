QWEN TTS PROJECT PLANNING
=========================

PROJECT OBJECTIVE:
- Setup Qwen Text-to-Speech (TTS) in a Docker container
- Create a REST API to call the TTS service
- Build a web UI for users to interact with the service

---

ARCHITECTURE OVERVIEW:
======================

1. DOCKER CONTAINER (Backend Service)
   - Qwen TTS model (inference engine)
   - Lightweight Python service to handle requests
   - GPU support for faster processing
   - Create a file like device_utils.py (detect whether a GPU is available, and fall back to CPU)
   - Volume mounts for model weights and outputs

2. REST API (FastAPI/Flask)
   - Endpoints:
     * POST /api/tts - Convert text to speech
     * GET /api/voices - List available voices
     * GET /api/status - Health check
   - Input: text, voice_id, language, speed, pitch
   - Output: Audio file (WAV, MP3, or stream)
   - Error handling & validation

3. WEB UI (React js)
   - Text input field
   - Voice selection dropdown
   - Upload Audio/record to for custom voice clone
   - Language selector
   - Speed/pitch controls
   - Play button
   - Download audio button
   - Real-time feedback/loading states

---

DOCKER SETUP STRATEGY:
=====================

Dockerfile Structure:
  1. Base image: python:3.10-slim or pytorch base image
  2. Install system dependencies (ffmpeg, libsndfile1)
  3. Install Python packages (torch, qwen-tts, fastapi, uvicorn)
  4. Download/cache Qwen TTS model weights
  5. Copy API code into container
  6. Expose port (e.g., 8000)
  7. Health check configured
  8. Entry point: uvicorn server

Docker Compose:
  - Service for Qwen TTS API
  - Volume mounts:
    * /models - for model weights (persistent)
    * /output - for generated audio files
    * /logs - for application logs
  - Port mapping: 8000:8000
  - GPU support (if available)
  - Environment variables for configuration

---

API DESIGN (FastAPI):
====================

Technology Stack:
  - Framework: FastAPI (async, automatic OpenAPI docs)
  - Server: Uvicorn
  - Audio handling: librosa, scipy, pydub, soundfile
  - Task queue: Celery (recommended for production with concurrent requests)

Key Endpoints:

1. POST /api/tts/custom-voice (Pre-built voices)
   Request JSON:
   {
     "text": "Hello, world!",
     "voice": "Vivian",  # or Ryan, Aiden, Sohee, etc.
     "language": "Chinese",  # or "Auto"
     "speed": 1.0,
     "pitch": 1.0,
     "instruction": "optional emotion/tone control"
   }
   Response: Binary audio (WAV/MP3) or JSON with audio URL

2. POST /api/tts/voice-design (Describe custom voice)
   Request JSON:
   {
     "text": "Your text here",
     "language": "English",
     "voice_description": "A warm, elderly male voice with a slight accent"
   }
   Response: Generated audio with designed voice

3. POST /api/tts/voice-clone (Clone from reference)
   Request FormData:
   {
     "text": "Text to synthesize",
     "reference_audio": [binary audio file],
     "reference_text": "Transcription of reference audio",
     "language": "English",
     "x_vector_only_mode": false  # optional: speaker embedding only
   }
   Response: Cloned voice audio

4. GET /api/voices
   Returns: List of available voice profiles with descriptions
   {
     "voices": [
       {
         "id": "Vivian",
         "name": "Vivian",
         "description": "Bright, slightly edgy young female voice",
         "native_language": "Chinese",
         "supported_languages": ["Chinese", "English", "Japanese", ...]
       },
       ...
     ]
   }

5. GET /api/languages
   Returns: Supported languages and their codes
   {
     "languages": [
       {"code": "Chinese", "name": "Mandarin Chinese"},
       {"code": "English", "name": "English"},
       ...
     ]
   }

6. GET /api/health
   Returns: Service status and model info
   {
     "status": "healthy",
     "models": {
       "tokenizer": "loaded",
       "custom_voice": "loaded",
       "voice_design": "not_loaded",
       "voice_clone": "loaded"
     },
     "gpu_available": true,
     "dtype": "bfloat16"
   }

7. POST /api/tokenize (Optional: for advanced use)
   Request: Binary audio
   Response: Tokenized representation

Design Considerations for Production:
  - Batch request handling (multiple texts in single request)
  - Async/await for streaming responses
  - Request queuing with background tasks
  - Audio format negotiation (WAV, MP3, OGG)
  - Streaming audio responses for low-latency scenarios
  - Rate limiting and authentication
  - Cache for frequently requested audio
  - Error handling for invalid languages/speakers

---

WEB UI DESIGN:
==============

Frontend Technology:
  - Option 1: Simple HTML + Vanilla JS (lightweight)
  - Option 2: React (more interactive, better state management)
  - Option 3: Vue.js (middle ground)

Core Features:
  1. Text Input Area
     - Multi-line textarea
     - Character counter (max 1000 chars)
     - Clear button

  2. Settings Panel
     - Voice selector dropdown
     - Language selector
     - Speed slider (0.5 - 2.0)
     - Pitch slider (0.5 - 2.0)

  3. Audio Player
     - Play/pause controls
     - Progress bar
     - Download button
     - Copy share URL (optional)

  4. Status Indicators
     - Loading spinner during processing
     - Error messages
     - Success notifications
     - Processing time display

  5. Responsive Design
     - Mobile-friendly layout
     - Touch-friendly controls
     - Dark/light mode support (optional)

---

DEVELOPMENT PHASES:
===================

PHASE 1: Core API (Week 1)
  [ ] Install Qwen TTS locally
  [ ] Create FastAPI application
  [ ] Implement /api/tts endpoint
  [ ] Test with curl/Postman
  [ ] Add error handling

PHASE 2: Docker Setup (Week 2)
  [ ] Create Dockerfile with Qwen TTS
  [ ] Build Docker image
  [ ] Test container locally
  [ ] Create docker-compose.yml
  [ ] Setup volume mounts
  [ ] GPU support (optional)

PHASE 3: Complete API (Week 2)
  [ ] Add /api/voices endpoint
  [ ] Add /api/health endpoint
  [ ] Add logging
  [ ] Add rate limiting
  [ ] API documentation

PHASE 4: Web UI (Week 3)
  [ ] Create HTML/CSS structure
  [ ] Implement text input
  [ ] Add voice selection
  [ ] Implement settings controls
  [ ] Connect to API
  [ ] Add audio player
  [ ] Test with different browsers

PHASE 5: Polish & Deployment (Week 4)
  [ ] Error handling in UI
  [ ] Loading states
  [ ] Performance optimization
  [ ] Security hardening
  [ ] Deploy to cloud (optional)
  [ ] Documentation

---

TECHNOLOGY RECOMMENDATIONS:
===========================

Backend:
  - Python 3.12+ (Qwen3-TTS official requirement)
  - FastAPI (lightweight, async)
  - Uvicorn (ASGI server)
  - Pydantic (data validation)
  - Librosa (audio processing)
  - PyTorch with torch.bfloat16 support
  - soundfile (audio I/O)
  - qwen-tts (official package from PyPI)

Frontend:
  - HTML5
  - CSS3 (or Tailwind CSS)
  - Vanilla JavaScript or React
  - Fetch API for HTTP requests

DevOps:
  - Docker & Docker Compose
  - GitHub Actions (CI/CD)
  - Environment variables for config

---

EXPECTED CHALLENGES & SOLUTIONS:
================================

1. Model Size
   Problem: Qwen TTS models can be large (1-5GB)
   Solution: Use Docker layer caching, volume mounts for persistence

2. GPU Memory
   Problem: TTS inference on CPU is slow
   Solution: Configure Docker for GPU access (NVIDIA runtime)

3. Audio Processing
   Problem: Multiple audio formats to support
   Solution: Use librosa + pydub for format conversion

4. Latency
   Problem: First request is slow (model loading)
   Solution: Pre-load model on container startup

5. Concurrent Requests
   Problem: Multiple users requesting TTS simultaneously
   Solution: Implement task queue (Celery) or async processing

6. CORS Issues
   Problem: UI on different domain accessing API
   Solution: Configure CORS headers in FastAPI

---

QWEN3-TTS SPECIFIC DETAILS:
===========================

Model Variants Available:
  1. Qwen3-TTS-12Hz-1.7B-CustomVoice
     - Supports 9 premium pre-built voices (Vivian, Serena, Uncle_Fu, Dylan, Eric, Ryan, Aiden, Ono_Anna, Sohee)
     - Each voice has specific language strengths
     - Supports natural language instructions for voice control
     - Supports: Chinese, English, Japanese, Korean, German, French, Russian, Portuguese, Spanish, Italian

  2. Qwen3-TTS-12Hz-1.7B-VoiceDesign
     - Create custom voices from natural language descriptions
     - Example: "体现撒娇稚嫩的萝莉女声，音调偏高且起伏明显..." (voice characteristics in text)
     - Supports all 10 languages

  3. Qwen3-TTS-12Hz-1.7B-Base & 0.6B-Base
     - Voice cloning model (3-second reference audio required)
     - Can be fine-tuned for custom applications
     - Base for advanced voice cloning workflows
     - Lightweight 0.6B option for resource-constrained environments

  4. Qwen3-TTS-Tokenizer-12Hz
     - Separate model for audio encoding/decoding
     - Encodes audio to codes for transport or training
     - 12Hz sample rate (key differentiator)

Core Features:
  ✓ Low-latency streaming generation (as low as 97ms end-to-end)
  ✓ Multi-codebook LM architecture (bypasses traditional LM+DiT bottlenecks)
  ✓ Paralinguistic information preservation (emotions, tone, prosody)
  ✓ Dual-Track hybrid streaming for real-time scenarios
  ✓ Batch inference support for efficiency
  ✓ Auto language detection or explicit language specification

Installation Requirements:
  # Essential
  pip install -U qwen-tts
  
  # Highly Recommended for GPU optimization
  pip install -U flash-attn --no-build-isolation
  # Note: FlashAttention 2 reduces GPU memory significantly
  # Works with torch.float16 or torch.bfloat16 only
  # Check compatibility: https://github.com/Dao-AILab/flash-attention
  
  # For limited RAM (< 96GB) and many CPU cores
  MAX_JOBS=4 pip install -U flash-attn --no-build-isolation

Environment Setup:
  conda create -n qwen3-tts python=3.12 -y
  conda activate qwen3-tts
  pip install -U qwen-tts
  pip install -U flash-attn --no-build-isolation

API Usage Patterns (from official docs):

1. CustomVoice (Pre-built Voices):
   from qwen_tts import Qwen3TTSModel
   model = Qwen3TTSModel.from_pretrained(
       "Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice",
       device_map="cuda:0",
       dtype=torch.bfloat16,
       attn_implementation="flash_attention_2",
   )
   wavs, sr = model.generate_custom_voice(
       text="...",
       language="Chinese",  # or "Auto" for auto-detection
       speaker="Vivian",    # pre-defined voice
       instruct="optional voice control instruction"
   )

2. VoiceDesign (Describe Your Voice):
   model = Qwen3TTSModel.from_pretrained(
       "Qwen/Qwen3-TTS-12Hz-1.7B-VoiceDesign",
       device_map="cuda:0",
       dtype=torch.bfloat16,
       attn_implementation="flash_attention_2",
   )
   wavs, sr = model.generate_voice_design(
       text="...",
       language="Chinese",
       instruct="natural language description of voice characteristics"
   )

3. Voice Clone (From Reference Audio):
   model = Qwen3TTSModel.from_pretrained(
       "Qwen/Qwen3-TTS-12Hz-1.7B-Base",
       device_map="cuda:0",
       dtype=torch.bfloat16,
       attn_implementation="flash_attention_2",
   )
   wavs, sr = model.generate_voice_clone(
       text="...",
       language="English",
       ref_audio="path/or/url/or/numpy_array",  # 3+ seconds recommended
       ref_text="transcription of reference audio"
   )
   # Advanced: Reuse voice clone prompts to avoid re-extracting features
   prompt = model.create_voice_clone_prompt(ref_audio=..., ref_text=...)

4. Voice Design → Clone Workflow (Hybrid):
   # Step 1: Design a voice using VoiceDesign model
   ref_wavs, sr = design_model.generate_voice_design(text=..., instruct=...)
   # Step 2: Create reusable clone prompt from designed voice
   prompt = clone_model.create_voice_clone_prompt(ref_audio=(ref_wavs[0], sr), ref_text=...)
   # Step 3: Use for multiple generations without re-extraction
   wavs, sr = clone_model.generate_voice_clone(text=..., voice_clone_prompt=prompt)

5. Tokenizer Operations (Audio Encode/Decode):
   from qwen_tts import Qwen3TTSTokenizer
   tokenizer = Qwen3TTSTokenizer.from_pretrained(
       "Qwen/Qwen3-TTS-Tokenizer-12Hz",
       device_map="cuda:0",
   )
   enc = tokenizer.encode("audio_file_or_url")
   wavs, sr = tokenizer.decode(enc)

Pre-built Voice Profiles (CustomVoice model):
  - Vivian: Bright, edgy young female (Chinese native)
  - Serena: Warm, gentle young female (Chinese native)
  - Uncle_Fu: Seasoned male, low mellow timbre (Chinese native)
  - Dylan: Youthful Beijing male, clear natural (Beijing Dialect)
  - Eric: Lively Chengdu male, husky brightness (Sichuan Dialect)
  - Ryan: Dynamic male, strong rhythmic (English native)
  - Aiden: Sunny American male, clear midrange (English native)
  - Ono_Anna: Playful Japanese female, light nimble (Japanese native)
  - Sohee: Warm Korean female, rich emotion (Korean native)

Model Downloads:
  # Hugging Face (recommended globally)
  huggingface-cli download Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice --local-dir ./models/
  
  # ModelScope (recommended for Mainland China)
  modelscope download --model Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice --local_dir ./models/

Official Web UI Demo:
  # Built-in Gradio interface
  qwen-tts-demo Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice --ip 0.0.0.0 --port 8000
  
  # For Voice Clone model (HTTPS required for microphone access)
  openssl req -x509 -newkey rsa:2048 -keyout key.pem -out cert.pem -days 365 -nodes -subj "/CN=localhost"
  qwen-tts-demo Qwen/Qwen3-TTS-12Hz-1.7B-Base \
    --ip 0.0.0.0 --port 8000 \
    --ssl-certfile cert.pem \
    --ssl-keyfile key.pem \
    --no-ssl-verify

Alternative: vLLM-Omni Support
  - Official day-0 support from vLLM project
  - Currently offline inference only (online serving coming)
  - Better performance and optimization for Qwen3-TTS
  - GitHub: https://github.com/vllm-project/vllm-omni

Commercial API Option: DashScope
  - Alibaba's managed API service
  - Supports CustomVoice, VoiceDesign, and Voice Clone models
  - Real-time streaming capabilities
  - Documentation: https://help.aliyun.com/zh/model-studio/

Performance Characteristics:
  - End-to-end latency: as low as 97ms (streaming mode)
  - Sample rate: 12Hz tokenization (Qwen3-TTS-Tokenizer-12Hz)
  - Batch inference: Supported for efficiency
  - GPU Memory: FlashAttention 2 significantly reduces usage
  - Max tokens: Configurable via max_new_tokens parameter (default 2048)
  - Data type: Recommend torch.bfloat16 for best performance/quality tradeoff

Fine-tuning Support:
  - Base models (0.6B/1.7B-Base) can be fine-tuned
  - Reference: /finetuning directory in official repo
  - Useful for domain-specific voice adaptation

Testing & Evaluation:
  - Official evaluation benchmarks available
  - Tested with dtype=torch.bfloat16, max_new_tokens=2048
  - Seed-Test and InstructTTS-Eval test sets provided
  - Auto language detection validation included

---

CRITICAL ADDITIONS FROM QWEN3-TTS RESEARCH:
============================================

Important Implementation Details:

1. Model Loading Strategy
   - Pre-load models on container startup (avoid 97ms+ latency on first request)
   - Use device_map="cuda:0" for GPU acceleration
   - Load with dtype=torch.bfloat16 for optimal performance
   - Enable flash_attention_2 to reduce GPU memory significantly

2. Batch Processing Is Critical
   - The models support batch inference natively
   - Can process multiple texts with different speakers/languages in one call
   - Significantly improves throughput under concurrent load
   - Implement batch endpoint: POST /api/tts/batch

3. Memory Management
   - Small model: 0.6B option available (for edge devices/low-resource environments)
   - Large model: 1.7B (better quality, higher resource requirements)
   - FlashAttention 2 is HIGHLY RECOMMENDED for reducing GPU memory
   - If < 96GB RAM, use: MAX_JOBS=4 pip install -U flash-attn

4. Language Handling
   - Supports 10 languages: Chinese, English, Japanese, Korean, German, French, Russian, Portuguese, Spanish, Italian
   - Can set language="Auto" for automatic detection from text
   - Each pre-built voice has optimal native language (check voice table)
   - Strongly recommend explicit language for best quality

5. Voice Profiles (CustomVoice Model)
   - 9 pre-defined voices available
   - Cannot create unlimited voices with CustomVoice (fixed set)
   - For custom voices: Use VoiceDesign or Voice Clone models instead
   - Voice profile table: Vivian, Serena, Uncle_Fu, Dylan, Eric, Ryan, Aiden, Ono_Anna, Sohee

6. Voice Cloning Best Practices
   - Reference audio should be 3+ seconds minimum
   - Reference text MUST be accurate transcription
   - Can create reusable voice_clone_prompt for efficiency
   - Avoid re-extracting features for same voice across multiple calls
   - API: model.create_voice_clone_prompt() → reuse in multiple generate_voice_clone() calls

7. Streaming vs Non-Streaming
   - Dual-Track architecture supports both seamlessly
   - First audio packet available in ~97ms (streaming mode)
   - Use for real-time interactive scenarios
   - Important for user-facing API responsiveness

8. Voice Instruction/Control
   - Use natural language instructions for tone/emotion control
   - Example: "用特别愤怒的语气说" (speak with extreme anger)
   - Example: "Speak in an incredulous tone, but with panic creeping in"
   - Works best with language-aware instructions

9. Official Web UI / Demo
   - Built-in Gradio demo available: qwen-tts-demo command
   - Can serve as reference for UI design
   - For Voice Clone model: HTTPS required (microphone permission)
   - OpenSSL certificates needed for production Voice Clone serving

10. Fine-tuning Capability
    - Base models (0.6B and 1.7B) support fine-tuning
    - Possible use cases: domain-specific adaptation, accent training
    - Reference: /finetuning directory in official repository
    - Useful for enterprise customization

11. Alternative Deployment: vLLM-Omni
    - Official vLLM-Omni support for Qwen3-TTS
    - Better performance and optimization than direct qwen-tts package
    - Currently offline inference only
    - Consider for high-performance production deployments

12. Alternative API Service: Alibaba DashScope
    - Managed API service by Alibaba (Qwen's parent company)
    - Supports all models: CustomVoice, VoiceDesign, Voice Clone
    - Real-time streaming capabilities
    - Consider for serverless/managed deployment
    - Eliminates need to manage GPU resources

13. Audio Input Format Support
    - Accepts: file paths, URLs, base64 strings, numpy arrays
    - Highly flexible for API design
    - Streaming upload support possible
    - Consider supporting multiple input methods in API

14. Output Audio Format
    - Default: WAV format
    - Can be converted to MP3, OGG using pydub/ffmpeg
    - Streaming response support for low-latency scenarios
    - Consider audio format negotiation in API headers

15. Performance Optimization
    - Batch processing: 2-5x throughput improvement
    - Model caching: Eliminates reload overhead
    - FlashAttention 2: 30-50% GPU memory reduction
    - Streaming: Lower perceived latency for users
    - Request queue with Celery: Handle burst traffic

16. Error Handling Critical Points
    - Invalid voice names → clear error with available voices list
    - Unsupported language/voice combinations → validation
    - Reference audio too short/invalid for cloning → user guidance
    - GPU out of memory → graceful degradation strategy
    - Model not loaded → meaningful error messages

17. Testing Considerations
    - Official evaluation benchmarks available
    - Test sets: Seed-Test, InstructTTS-Eval provided
    - Benchmark with dtype=torch.bfloat16, max_new_tokens=2048
    - Test batch vs single inference performance
    - Test with auto language detection vs explicit language

---

FILE STRUCTURE:

qwentts/
├── backend/
│   ├── app.py (FastAPI application)
│   ├── requirements.txt
│   ├── models/ (model handling)
│   ├── routes/ (API endpoints)
│   └── utils/ (helper functions)
├── frontend/
│   ├── index.html
│   ├── styles.css
│   ├── script.js
│   └── assets/
├── docker/
│   ├── Dockerfile
│   ├── docker-compose.yml
│   └── .dockerignore
├── docs/
│   ├── API_DOCS.md
│   └── SETUP.md
└── README.md

---

NEXT STEPS:
===========

1. Research Qwen TTS installation requirements
2. Create Docker setup and test locally
3. Build FastAPI backend with TTS integration
4. Develop simple web UI
5. Integration testing
6. Documentation and deployment

